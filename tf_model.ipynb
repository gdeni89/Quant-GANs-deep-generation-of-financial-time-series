{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN MODEL implemented in Tensor Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wiese et al., Quant GANs: Deep Generation of Financial Time Series, 2019](https://arxiv.org/abs/1907.06673)\n",
    "\n",
    "For both the generator and the discriminator we used TCNs with skip connections. Inside the TCN architecture temporal blocks were used as block modules. A temporal block consists of two dilated causal convolutions and two PReLUs (He et al., 2015) as activation functions. The primary benefit of using temporal blocks is to make the TCN more expressive by increasing the number of non-linear operations in each block module. A complete definition is given below.\n",
    "\n",
    "**Definition B.1 (Temporal block)**. Let $N_I, N_H, N_O ∈ \\Bbb{N}$ denote the input, hidden and output dimension and let $D,K ∈ \\mathbb{N}$ denote the dilation and the kernel size. Furthermore, let $w_1, w_2$ be two dilated causal convolutional layers with arguments $(N_I, N_H, K, D)$  and $(N_H,N_O,K,D)$ respectively and\n",
    "let $φ_1, φ_2 : \\mathbb{R} → \\mathbb{R}$ be two PReLUs. The function $f : \\mathbb{R}^{N_I×(2D(K−1)+1)} → \\mathbb{R}^{N_O}$ defined by\n",
    "$$f(X) = φ_2 ◦ w_2 ◦ φ_1 ◦ w_1(X)$$\n",
    "is called temporal block with arguments $(N_I,N_H,N_O,K,D)$.\n",
    "\n",
    "The TCN architecture used for the generator and the discriminator in the pure TCN and C-SVNN model is illustrated in Table 3. Table 4 shows the input, hidden and output dimensions of the different models. Here, G abbreviates the generator and D the discriminator. Note that for all models, except the generator of the C-SVNN, the hidden dimension was set to eighty. The kernel size of each temporal block, except the first one, was two. Each TCN modeled a RFS of 127."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-ik58{background-color:#2f2f2f;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<h3>Table 3</h3>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-ik58\">Module Name</th>\n",
    "    <th class=\"tg-ik58\">Arguments</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 1</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 1, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 2</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 3</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 2)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 4</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 4)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 5</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 8)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 6</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 16)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 7</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 32)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">1 x 1 Convolution</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>H</sub>, N<sub>O</sub>, 1, 1)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;background-color:#2f2f2f;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<h3>Table 4</h3>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">Models</th>\n",
    "    <th class=\"tg-0lax\">PureTCN-G</th>\n",
    "    <th class=\"tg-0lax\">Pure TCN-D<br></th>\n",
    "    <th class=\"tg-0lax\">C-SVNN-G</th>\n",
    "    <th class=\"tg-0lax\">C-SVNN_D</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>I</sub></td>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>H</sub></td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "    <td class=\"tg-0lax\">50<br></td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>O</sub></td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12591/3928169354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpectralNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfixed_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "\t!pip install tensorflow-addons\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import PReLU, Conv1D,  Add, Input, Cropping2D, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.layers import BatchNormalization\n",
    "from tensorflow_addons.layers import SpectralNormalization\n",
    "\n",
    "fixed_filters = 80\n",
    "receptive_field_size = 127\n",
    "block_size = 2\n",
    "\n",
    "def add_temporal_block(previous, skip, kernel_size, dilation, cropping):\n",
    "    \"\"\"Creates a temporal block.\n",
    "    Args:\n",
    "        previous (tensorflow.keras.layers.Layer): previous layer to attach to on standard path.\n",
    "        skip (tensorflow.keras.layers.Layer): skip layer to attach to on the skip path. Use None for intiation.\n",
    "        kernel_size (int): kernel size along temporal axis of convolution layers within the temporal block.\n",
    "        dilation (int): dilation of convolution layers along temporal axis within the temporal block.\n",
    "    Returns:\n",
    "        tuple of tensorflow.keras.layers.Layer: Output layers belonging to (normal path, skip path).\n",
    "    \"\"\"\n",
    "    print(f\"kernel_size: {kernel_size}  dilation: {dilation}, fixed_filters: {fixed_filters} cropping: {cropping}\")\n",
    "    # Identity mapping so that we hold a valid reference to previous\n",
    "    block = Lambda(lambda x: x)(previous)\n",
    "\n",
    "    for _ in range(block_size):\n",
    "        convs = []\n",
    "        prev_block= Lambda(lambda x: x)(block)\n",
    "        convs.append(SpectralNormalization(Conv1D(fixed_filters, (kernel_size), dilation_rate=(dilation,)))(block))\n",
    "\n",
    "        if len(convs) > 1:\n",
    "            block = Concatenate(axis=1)(convs) \n",
    "        else:\n",
    "            block = convs[0]\n",
    "        block = BatchNormalization(axis=3, momentum=.9, epsilon=1e-4, renorm=True, renorm_momentum=.9)(block)\n",
    "        block = PReLU(shared_axes=[2, 3])(block)\n",
    "\n",
    "    # As layer output gets smaller, we need to crop less before putting output\n",
    "    # on the skip path. We cannot infer this directly as tensor shapes may be variable.\n",
    "    drop_left = block_size * (kernel_size - 1) * dilation\n",
    "    cropping += drop_left\n",
    "\n",
    "    if skip is None:\n",
    "        previous = Conv1D(fixed_filters, 1)(previous)\n",
    "    # add residual connections\n",
    "    out = Add()([Cropping2D(cropping=((0,0), (drop_left, 0)))(previous), block])\n",
    "    # crop from left side for skip path\n",
    "    skip_out = Cropping2D(cropping=((0,0), (receptive_field_size-1-cropping, 0)))(out)\n",
    "    # add current output with 1x1 conv to skip path\n",
    "    if skip is not None:\n",
    "        skip_out = Add()([skip, SpectralNormalization(Conv1D(fixed_filters, 1))(skip_out)])\n",
    "    else:\n",
    "        skip_out = SpectralNormalization(Conv1D(fixed_filters, 1))(skip_out)\n",
    "\n",
    "    return PReLU(shared_axes=[2, 3])(out), skip_out, cropping\n",
    "\t\n",
    "def TCN(input_dim):\n",
    "    \"\"\"Causal temporal convolutional network with skip connections.\n",
    "       This network uses 1D convolutions in order to model multiple timeseries co-dependency.\n",
    "    Args:\n",
    "        input_dim (list): Input dimension of the shape (timesteps, number of features). Timesteps may be None for variable length timeseries. \n",
    "    Returns:\n",
    "        tensorflow.keras.models.Model: a non-compiled keras model.\n",
    "    \"\"\" \n",
    "    # Number of dilations in order to use for the temporal blocks.\n",
    "    dilations = np.array([1, 2, 4, 8, 16, 32])\n",
    "\n",
    "    input_dim.insert(0,1)\n",
    "    print(f\"input_dim: {input_dim}\")\n",
    "    input_layer = Input(shape=input_dim)\n",
    "    cropping = 0\n",
    "    assert (sum(dilations) * block_size + 1) == 127, \"Paper specifies receptive field size should be 127\"\n",
    "    \n",
    "    prev_layer, skip_layer, _ = add_temporal_block(input_layer, None, 1, 1, cropping)\n",
    "                \n",
    "    for dilation in dilations:\n",
    "        prev_layer, skip_layer, cropping = add_temporal_block(prev_layer, skip_layer, 2, dilation, cropping)\n",
    "\n",
    "    output_layer = PReLU(shared_axes=[2, 3])(skip_layer)\n",
    "    output_layer = SpectralNormalization(Conv1D(fixed_filters, kernel_size=1))(output_layer)\n",
    "    output_layer = PReLU(shared_axes=[2, 3])(output_layer)\n",
    "    output_layer = SpectralNormalization(Conv1D(1, kernel_size=1))(output_layer)\n",
    "\n",
    "    return Model(input_layer, output_layer)\n",
    "\n",
    "generator = TCN([None, 3])\n",
    "discriminator = TCN([receptive_field_size, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d89b3f520990f67813a536e0845046cd8eba1f701f32f0e9331279df485a6ae1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
