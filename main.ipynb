{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd6e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71bb3091",
   "metadata": {},
   "source": [
    "# Quant GANs: Deep Generation of Financial Time Series\n",
    "In this notebook we replicate the different steps developped in [Wiese et al., Quant GANs: Deep Generation of Financial Time Series, 2019](https://arxiv.org/abs/1907.06673) to generate synthetic financial series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72faa374",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Timeseries: Fetching and Characteristics\n",
    "In the first chapter, we use the yfinance module to obtain financial series. In this notebook we focus on equity series. We illustrate the characteristics of the financial series that we seek to replicate in synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90f6b6",
   "metadata": {
    "title": "Times Series"
   },
   "outputs": [],
   "source": [
    "# We use yfinance to download our targeted financial variables, the daily close price for the cac0.\n",
    "!pip install yfinance -q\n",
    "os.chdir('/home/davidg/Documents/Cours/MLforFinance/temporalCN')\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "os.chdir('.')\n",
    "os.makedirs('./figure/',exist_ok=True)\n",
    "os.makedirs('./table/',exist_ok=True)\n",
    "\n",
    "tickers_list = ['^FCHI',]\n",
    "names  = ['CAC40',]\n",
    "n_dict = dict(zip(tickers_list,names))\n",
    "df     = pd.DataFrame(yf.download(tickers_list,'1990-1-1')['Adj Close'])\n",
    "df     = df.rename(columns={'Adj Close':names[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4a366",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We plot our series of interest, the series is non stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6a97a",
   "metadata": {
    "title": "Raw Series"
   },
   "outputs": [],
   "source": [
    "ax = df.plot(figsize=(10,5), legend=False)\n",
    "ax.grid(True)\n",
    "plt.savefig('./figure/CAC40.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970c507",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The next plots show the returns for the CAC40 and its autocorrelation at various lag orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d07571",
   "metadata": {
    "title": "Return process"
   },
   "outputs": [],
   "source": [
    "from preprocess.acf import acf, rolling_window\n",
    "returns = df.shift(1)/df - 1\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(returns[200:400])\n",
    "axs[0].set_title('Returns')\n",
    "axs[1].plot(acf(returns, 120)[1:])\n",
    "axs[1].set_title('Return Autocorrelation')\n",
    "\n",
    "for ax in axs: ax.grid(True)\n",
    "plt.setp(axs[0], xlabel='Day')\n",
    "plt.setp(axs[0].get_xticklabels(), rotation=45, ha='right')\n",
    "plt.setp(axs[1], xlabel='Lag (days)')\n",
    "plt.setp(axs[0], ylabel='Relative Returns')\n",
    "plt.setp(axs[1], ylabel='Autocorrelation')\n",
    "axs[1].set_ylabel('correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figure/returns_val_corr.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da170726",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Log Returns as Financial Data\n",
    "We also present log returns. They  have various advantages over simple returns (they are approximately normal, they are equal to cumulative return of the asset/portfolio and they are symmetric).\n",
    "We calculate the log return series as,\n",
    "$$r_t = \\log\\biggl(\\frac{s_t}{s_{t-1}}\\biggr) \\text { for all } t \\in \\{1, ..., T\\}.$$\n",
    "Here we illustrate some of the stylized facts from [Chakraborti et al., 2011].\n",
    "First, log returns are not autocorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb1953",
   "metadata": {
    "title": "Log return process"
   },
   "outputs": [],
   "source": [
    "log_returns = np.log(df/df.shift(1))[1:].to_numpy().reshape(-1, 1)\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(log_returns[200:400])\n",
    "axs[0].set_title('log returns')\n",
    "axs[1].plot(acf(log_returns, 50))\n",
    "axs[1].set_title('Identity log returns')\n",
    "\n",
    "for ax in axs: ax.grid(True)\n",
    "plt.setp(axs[0], xlabel='Day')\n",
    "plt.setp(axs[1], xlabel='Lag (days)')\n",
    "plt.setp(axs[0], ylabel='relative return')\n",
    "plt.setp(axs[1], ylabel='autocorrelation')\n",
    "axs[1].set_ylabel('correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figure/log_returns_val_corr.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23879d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    " Second, the volatility of asset returns negatively correlates with the return process, something known as leverage effect. Here we show the autocorrelation of log returns with its lagged standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1a8dc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.plot(acf(log_returns, 125, le=True))\n",
    "ax.set_title('Leverage effect log returns')\n",
    "plt.setp(ax, xlabel='Lag (days)')\n",
    "plt.setp(ax, ylabel='autocorrelation')\n",
    "ax.set_ylabel('correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figure/leverage_effect.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f896",
   "metadata": {},
   "source": [
    "Prior to passing a realization of a financial time series $s_{0:T} ∈ \\mathbb{R}^{N_X×(T+1)}$ to the discriminator, the time series has to be preprocessed. The applied pipeline is described in the report. We briefly explain each of the steps taken. Note that all of the used transformations, excluding the rolling window, are invertible and thus, allow a series sampled from a log return NP to be post-processed by inverting the steps 1-4 to obtain the desired form. Also, observe that the pipeline includes the inverse Lambert W transformation as earlier discussed in subsection 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5f838",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The data need to be preprocessed before being brought to the model. We describe each step separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df1eca",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Step 1: Normalization\n",
    "For numerical reasons, we normalize the data in order to obtain a series with zero mean and unit variance, which is thoroughly derived in LeCun et al. (1998)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from preprocess.gaussianize import Gaussianize, norm\n",
    "\n",
    "gaussianize, standardScaler1, standardScaler2 = Gaussianize(), StandardScaler(), StandardScaler()\n",
    "log_returns_preprocessed = standardScaler2.fit_transform(gaussianize.fit_transform(standardScaler1.fit_transform(log_returns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5442c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Step 2: Inverse Lambert W transform\n",
    "The suggested transformation applied to the log returns of the CAC40 is displayed in Figure 10. It shows the standardized original distribution of the CAC40 log returns and the inverse Lambert W transformed log return distribution. Observe that the transformed standardized log return distribution in Figure 10b approximately follows the standard normal distribution and thereby circumvents the issue of not being able to generate the heavy-tail of the original distribution.\n",
    "\n",
    "# Step 3: Rolling window\n",
    "When considering a discriminator with receptive field size $T^{(d)}$, we apply a rolling window of corresponding length and stride one to the preprocessed log return sequence $r^{(ρ)}_t $. Hence, for $t∈\\{1,...,T −T^{(d)}\\}$ we define the sub-sequences $$r^{(t)}_{1:T^{(d)}} := r^{(ρ)}_{t:(T^{(d)}+t−1)} ∈  \\mathbb{R}^{N_Z×T^{(d)}}.$$\n",
    "\n",
    "Remark 6.1. Note that sliding a rolling window introduces a bias, since log returns at the beginning and end of the time series are under-sampled when training the Quant GAN. This bias can be corrected by using a (non-uniform) weighted sampling scheme when sampling batches from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57687134",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# We use a receptive field of size 127 as in the paper.\n",
    "log_returns_rolled = rolling_window(log_returns_preprocessed, 127)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c99afc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Because log returns exhibit fat tail and are more piked than gaussian variables, Lambert W transformation can be applied to bring log returns to gaussianity and improve the performances of methods meant for normalized Gaussian data.\n",
    "Here we show log returns before and after the lambert W inverse transform respectively (both standardized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82897ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 5), sharey=True, sharex=True)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "ax[0].hist(standardScaler1.transform(log_returns), bins=100, density=True)\n",
    "x_range = np.linspace(*ax[0].get_xlim(), num=1000)\n",
    "ax[0].plot(x_range, norm.pdf(x_range), linestyle='dashed')\n",
    "\n",
    "ax[1].hist(log_returns_preprocessed, bins=100, density=True)\n",
    "x_range = np.linspace(*ax[1].get_xlim(), num=1000)\n",
    "ax[1].plot(x_range, norm.pdf(x_range), linestyle='dashed')\n",
    "\n",
    "# add a grid to compare easily\n",
    "ax[0].yaxis.grid(True, alpha=0.5)\n",
    "ax[1].yaxis.grid(True, alpha=0.5);\n",
    "\n",
    "ax[1].legend(['Standard normal distribution', 'Standardized data'])\n",
    "ax[0].set_ylabel('Emperical probability density')\n",
    "ax[0].set_xlabel('Standardized log returns')\n",
    "ax[1].set_xlabel('Gaussianized standardized log returns')\n",
    "\n",
    "ax[0].set_xlim(-4, 4)\n",
    "plt.savefig('./figure/empirical_distributions.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a9aae",
   "metadata": {},
   "source": [
    "## Quant GAN model\n",
    "In this second part we construct the GAN model used to generate syntetic financial series with properties similar to those presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef0581",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Preprocessing\n",
    "Here we set the different parameters and options required for the implementation of Quant GAN. The different modeling choices for the model parameters are descibed in the paper and primarily driven by performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539744eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from model.torch_tcn import *\n",
    "\n",
    "# Set gpu if available\n",
    "if torch.cuda.is_available():      \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set default optmization parameters\n",
    "num_epochs = 10\n",
    "nz         = 3\n",
    "batch_size = 20\n",
    "seq_len    = 127\n",
    "clip       = 0.01\n",
    "lr         = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0e5bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Dataset\n",
    "We construct a torch dataloader to feed batches of data to the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98643453",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "class Loader32(Dataset):\n",
    "    '''Brings batches from df of length lenght sequentially.'''\n",
    "    def __init__(self, df, length):\n",
    "        assert len(df) >= length\n",
    "        self.df = df\n",
    "        self.length = length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.df[idx:idx+self.length]).reshape(-1, self.length).to(torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.df)-self.length, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642ad95",
   "metadata": {},
   "source": [
    "The target we try to replicate are the log returns preprocessed. Remember this is the log returns where we applied the following steps:\n",
    "- Normalize\n",
    "- Inverse Lambert W transform\n",
    "- Normalize\n",
    "- Rolling Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5812a76",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Here, we set some final options for the data loader, especially the size of the receptive field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_path = './trained/'\n",
    "file_name = 'CAC40_daily'\n",
    "\n",
    "sd                       = 80\n",
    "receptive_field_size     = 127  \n",
    "log_returns_preprocessed = rolling_window(log_returns_preprocessed, receptive_field_size)\n",
    "data_size = log_returns.shape[0]\n",
    "print(log_returns_preprocessed.shape)\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c753d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Hyperparamter Tuning\n",
    "In this section we optimize the GAN model, the objective function minimizes the wasserstein distance between the synthetic and the real financial series.\n",
    "We optimize over several paramter such as the learning rate, batch size and so on. Here we use minimalist settings given the time it takes to compute the GAN model. Ideally we would use more involved settings (number of epochs and so on) and would hypertune over a wider range of parameters such as the stride of the convolution networks, the number of layer and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d5827",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Hypertuning"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance, norm, kurtosis, skew, skewtest, kurtosistest\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "# Make score\n",
    "def compute_emd(generator):\n",
    "    ''' Compute the wasserstein distance between real and synthetic data given a generator.'''\n",
    "    generator.eval()\n",
    "    noise = torch.randn(sd,3,receptive_field_size).to(device)\n",
    "    y     = generator(noise).cpu().detach().squeeze();\n",
    "\n",
    "    y = (y - y.mean(axis=0))/y.std(axis=0)\n",
    "    y = standardScaler2.inverse_transform(y)\n",
    "    y = np.array([gaussianize.inverse_transform(np.expand_dims(x, 1)) for x in y]).squeeze()\n",
    "    y = standardScaler1.inverse_transform(y)\n",
    "\n",
    "    # some basic filtering to redue the tendency of GAN to produce extreme returns\n",
    "    y  = y[(y.max(axis=1) <= 2 * log_returns.max()) & (y.min(axis=1) >= 2 * log_returns.min())]\n",
    "    y -= y.mean()\n",
    "\n",
    "    windows   = pd.Series([1, 5, 20, 100], name='window size')\n",
    "    EMDscores = np.zeros(len(windows))\n",
    "\n",
    "    for i in range(len(windows)):\n",
    "        real_dist = rolling_window(log_returns, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "        fake_dist = rolling_window(y.T, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "        \n",
    "        EMDscores[i] = wasserstein_distance(real_dist, fake_dist)\n",
    "\n",
    "    return EMDscores.sum()\n",
    "\n",
    "def train_tune(param, tuning=True):\n",
    "    '''Compute the GAN for a given set of parameter.'''\n",
    "    # Allocate params\n",
    "    lr         = param['lr']\n",
    "    batch_size = param['batch_size']\n",
    "    sd         = param['sd']\n",
    "    dropout    = param['dropout']\n",
    "\n",
    "    seq_len = 8021\n",
    "\n",
    "    generator      = Generator(sd=sd,dropout=dropout).to(device)\n",
    "    discriminator  = Discriminator(seq_len,sd=sd,dropout=dropout).to(device)\n",
    "    \n",
    "    disc_optimizer = optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "    gen_optimizer  = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "\n",
    "    dataset    = Loader32(log_returns_preprocessed, 1)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    t = tqdm(range(num_epochs))\n",
    "    for epoch in t:\n",
    "        for idx, data in enumerate(dataloader, 0):\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            real = data.to(device)\n",
    "            batch_size, seq_len = real.size(0), real.size(1)\n",
    "            noise = torch.randn(batch_size, nz, seq_len, device=device)\n",
    "            fake  = generator(noise).detach()\n",
    "            real  = real.reshape(batch_size,1,-1)\n",
    "\n",
    "            disc_loss = -torch.mean(discriminator(real)) + torch.mean(discriminator(fake))\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            for dp in discriminator.parameters():\n",
    "                dp.data.clamp_(-clip, clip)\n",
    "    \n",
    "            if idx % 5 == 0:\n",
    "                generator.zero_grad()\n",
    "                gen_loss = -torch.mean(discriminator(generator(noise)))\n",
    "                gen_loss.backward()\n",
    "                gen_optimizer.step()       \n",
    "        t.set_description('Discriminator Loss: %.8f Generator Loss: %.8f' % (disc_loss.item(), gen_loss.item()))\n",
    "    if tuning:\n",
    "        return compute_emd(generator)\n",
    "    else:\n",
    "        return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6904b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "After having defined all the necessary function for the tuning we use optuna to perform a bayesian search over our hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbec7d4",
   "metadata": {
    "title": "Hypertuning with Optuna"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective_(trial):\n",
    "    # Objective function with param grid\n",
    "    param = {\n",
    "        'lr':         trial.suggest_loguniform('lr', 0.00001,0.01),\n",
    "        'batch_size': trial.suggest_int('batch_size', 8, 32),\n",
    "        'dropout':    trial.suggest_loguniform('dropout', 1e-8, 0.1),\n",
    "        'sd':         trial.suggest_int('sd', 40, 100)\n",
    "        \n",
    "    }\n",
    "    return train_tune(param)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_, n_trials=20, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8cd1c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Optimization Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8edee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c61891",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Here we create some synthetic series using the estimated GAN. To do so we create a log return series, and use the reverse transformations used to process the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61271b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimal model using hypertuning results\n",
    "generator = train_tune(trial.params, tuning=False)\n",
    "generator.eval()\n",
    "noise = torch.randn(sd,3,receptive_field_size).to(device)\n",
    "y     = generator(noise).cpu().detach().squeeze();\n",
    "\n",
    "y = (y - y.mean(axis=0))/y.std(axis=0)\n",
    "y = standardScaler2.inverse_transform(y)\n",
    "y = np.array([gaussianize.inverse_transform(np.expand_dims(x, 1)) for x in y]).squeeze()\n",
    "y = standardScaler1.inverse_transform(y)\n",
    "\n",
    "# some basic filtering to redue the tendency of GAN to produce extreme returns\n",
    "y  = y[(y.max(axis=1) <= 2 * log_returns.max()) & (y.min(axis=1) >= 2 * log_returns.min())]\n",
    "y -= y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef3c50",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This are the different generated timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88f5cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "ax.plot(np.cumsum(y[0:30], axis=1).T, alpha=0.75)\n",
    "ax.set_title('30 generated log return paths'.format(len(y)))\n",
    "ax.set_xlabel('days')\n",
    "ax.set_ylabel('Cumalative log return');\n",
    "plt.savefig('./figure/log_returns_series.png',dpi=300)\n",
    "plt.show()\n",
    "n_bins  = 50\n",
    "windows = [1, 5, 20, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6761a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Even with basic settings, their properties are close to the real data as shown in the following histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad2db9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
    "for i in range(len(windows)):\n",
    "    row = min(max(0, i-1), 1)\n",
    "    col = i % 2\n",
    "    real_dist = rolling_window(log_returns, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    fake_dist = rolling_window(y.T, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    axs[row, col].hist(np.array([real_dist, fake_dist], dtype='object'), bins=50, density=True)\n",
    "    axs[row,col].set_xlim(*np.quantile(fake_dist, [0.001, .999]))\n",
    "    \n",
    "    axs[row,col].set_title('{} day return distribution'.format(windows[i]), size=16)\n",
    "    axs[row,col].yaxis.grid(True, alpha=0.5)\n",
    "    axs[row,col].set_xlabel('Cumalative log return')\n",
    "    axs[row,col].set_ylabel('Frequency')\n",
    "\n",
    "axs[0,0].legend(['Historical returns', 'Synthetic returns'])\n",
    "plt.savefig('./figure/synthetic_distributions.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d149b79",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Here we present some of the autocorrelations for the synthetic data and compare it to their real counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb49331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
    "axs[0,0].plot(acf(log_returns, 100))\n",
    "axs[0,0].plot(acf(y.T, 100).mean(axis=1))\n",
    "axs[0,0].set_ylim(-0.1, 0.1)\n",
    "axs[0,0].set_title('Identity log returns')\n",
    "axs[0,1].plot(acf(log_returns**2, 100))\n",
    "axs[0,1].set_ylim(-0.05, 0.5)\n",
    "axs[0,1].plot(acf(y.T**2, 100).mean(axis=1))\n",
    "axs[0,1].set_title('Squared log returns')\n",
    "axs[1,0].plot(abs(acf(log_returns, 100, le=True)))\n",
    "axs[1,0].plot(abs(acf(y.T, 100, le=True).mean(axis=1)))\n",
    "axs[1,0].set_ylim(-0.05, 0.4)\n",
    "axs[1,0].set_title('Absolute')\n",
    "axs[1,1].plot(acf(log_returns, 100, le=True))\n",
    "axs[1,1].plot(acf(y.T, 100, le=True).mean(axis=1))\n",
    "axs[1,1].set_ylim(-0.2, 0.1)\n",
    "axs[1,1].set_title('Leverage effect')\n",
    "\n",
    "for ax in axs.flat: \n",
    "  ax.grid(True)\n",
    "  ax.axhline(y=0, color='k')\n",
    "  ax.axvline(x=0, color='k')\n",
    "plt.setp(axs, xlabel='Lag (number of days')\n",
    "\n",
    "plt.savefig('./figure/synthetic_leverage.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75533cc2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Evaluation of the synthtetic series\n",
    "This section shows different distance to assess the proximity of the synthetic series to their observed counterpart.\n",
    "The earth mover distance describes how much probability mass has to be moved to transform Ph into Pg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance, norm, kurtosis, skew, skewtest, kurtosistest\n",
    "\n",
    "windows   = pd.Series([1, 5, 20, 100], name='window size')\n",
    "EMDscores = np.zeros(len(windows))\n",
    "for i in range(len(windows)):\n",
    "    real_dist = rolling_window(log_returns, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    fake_dist = rolling_window(y.T, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    \n",
    "    EMDscores[i] = wasserstein_distance(real_dist, fake_dist)\n",
    "\n",
    "df_EMD = pd.DataFrame({'Earth Mover Distance' : EMDscores}, index=windows)\n",
    "with open(\"./table/EMD_Scores.tex\", \"w\") as fh:\n",
    "    fh.write(df_EMD.to_latex())\n",
    "df_EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [1, 5, 20, 100, 1000]\n",
    "n_bins  = 100\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(windows), figsize=(5*len(windows),5))\n",
    "for i in range(len(windows)):\n",
    "    real_dist = rolling_window(log_returns, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    ax[i].hist(real_dist, bins=n_bins, density=True)\n",
    "    x_range = np.linspace(*ax[i].get_xlim(), 10000)\n",
    "    ax[i].plot(x_range, norm.pdf(x_range, real_dist.mean(), real_dist.std()))\n",
    "    ax[i].set_xlim(*np.quantile(real_dist, [0.001, .999]))\n",
    "    ax[i].set_title('{} day returns'.format(windows[i]))\n",
    "    ax[i].yaxis.grid(True, alpha=0.5)\n",
    "    ax[i].set_xlabel('cumulative log return')\n",
    "    ax[i].set_ylabel('frequency')\n",
    "\n",
    "ax[0].legend(['$\\phi_{\\hat{\\mu}, \\hat{\\sigma}}(\\cdot)$', 'historical returns'], loc='upper left')\n",
    "plt.savefig('./figure/real_agg_gauss.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c56e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [1, 5, 20, 100, 100]\n",
    "n_bins = 100\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(windows), figsize=(5*len(windows),5))\n",
    "for i in range(len(windows)):\n",
    "    fake_dist = rolling_window(y.T, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    ax[i].hist(fake_dist, bins=n_bins, density=True)\n",
    "    x_range = np.linspace(*ax[i].get_xlim(), 10000)\n",
    "    ax[i].plot(x_range, norm.pdf(x_range, fake_dist.mean(), fake_dist.std()))\n",
    "    ax[i].set_xlim(*np.quantile(fake_dist, [0.001, .999]))\n",
    "    ax[i].set_title('{} day returns'.format(windows[i]))\n",
    "    ax[i].yaxis.grid(True, alpha=0.5)\n",
    "    ax[i].set_xlabel('cumulative log return')\n",
    "    ax[i].set_ylabel('frequency')\n",
    "\n",
    "ax[0].legend(['$\\phi_{\\hat{\\mu}, \\hat{\\sigma}}(\\cdot)$', 'synthetic returns'], loc='upper left')\n",
    "plt.savefig('./figure/fake_agg_gauss.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183284fd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "windows = pd.Series([1, 5, 20, 100], name='window size')\n",
    "stats_array = np.zeros((len(windows), 4))\n",
    "\n",
    "for i in range(len(windows)):\n",
    "    real_dist = rolling_window(log_returns, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).squeeze()\n",
    "    stats_array[i, 0] = skew(real_dist)\n",
    "    stats_array[i, 1] = skewtest(real_dist).pvalue\n",
    "    stats_array[i, 2] = kurtosis(real_dist)\n",
    "    stats_array[i, 3] = kurtosistest(real_dist).pvalue\n",
    "\n",
    "stats_df = pd.DataFrame(np.round(stats_array, 3), columns=['skewness', 'skewness p-value', 'kurtosis', 'kurtosis p-value'], index=windows)\n",
    "\n",
    "with open(\"./table/stats_real.tex\", \"w\") as fh:\n",
    "    fh.write(stats_df.to_latex())\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbf93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = pd.Series([1, 5, 20, 100], name='window size')\n",
    "stats_array = np.zeros((len(windows), 4))\n",
    "\n",
    "for i in range(len(windows)):\n",
    "    fake_di = rolling_window(y.T, windows[i], sparse = not (windows[i] == 1)).sum(axis=0).ravel()\n",
    "    stats_array[i, 0] = skew(real_dist)\n",
    "    stats_array[i, 1] = skewtest(real_dist).pvalue\n",
    "    stats_array[i, 2] = kurtosis(real_dist)\n",
    "    stats_array[i, 3] = kurtosistest(real_dist).pvalue\n",
    "\n",
    "stats_df = pd.DataFrame(np.round(stats_array, 3), columns=['skewness', 'skewness p-value', 'kurtosis', 'kurtosis p-value'], index=windows)\n",
    "\n",
    "with open(\"./table/stats_fake.tex\", \"w\") as fh:\n",
    "    fh.write(stats_df.to_latex())\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dist = log_returns.squeeze()\n",
    "fake_dist = y.ravel() \n",
    "\n",
    "loss_assymetry = lambda dist, x: np.mean((dist[np.abs(dist) > x] >= 0))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "x_range = np.linspace(0, np.quantile(np.abs(real_dist), .95), 50)\n",
    "ax.plot(x_range, [loss_assymetry(real_dist, x) for x in x_range], label='real returns')\n",
    "ax.plot(x_range, [loss_assymetry(fake_dist, x) for x in x_range], label='synthetic returns')\n",
    "ax.hlines(0.5, -1, 1, linestyles='dotted');\n",
    "ax.set_xlim(0, np.quantile(np.abs(real_dist), .95))\n",
    "\n",
    "ax.set_xlabel('$r_a$', size=14)\n",
    "ax.set_ylabel('$E(r > 0$ | $|r| > r_a)$', size=14)\n",
    "\n",
    "ax.grid(alpha=0.7)\n",
    "ax.legend()\n",
    "plt.savefig('./figure/gl_assymetry.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd0c22",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "x_real = np.sort(abs(real_dist[real_dist <= 0]))\n",
    "x_fake = np.sort(abs(fake_dist[fake_dist <= 0]))\n",
    "\n",
    "plt.plot(x_real, x_real.cumsum()/x_real.sum())\n",
    "plt.plot(x_fake, x_fake.cumsum()/x_fake.sum())\n",
    "\n",
    "ax.grid(which='minor', alpha=0.4)\n",
    "ax.grid(which='major', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(np.arange(0, 1.2, 0.2))\n",
    "ax.set_yticks(np.arange(0, 1.05, .05), minor=True)\n",
    "\n",
    "plt.xlim(0, max(x_fake))\n",
    "plt.ylim(0, 1.05)\n",
    "plt.plot([max(x_real), max(x_fake)], [1, 1], color='#1f77b4')\n",
    "ax.vlines(max(x_real), 0, 1.5, linestyles='dotted')\n",
    "\n",
    "ax.legend(['$|X_{real}|$ | $X_{real} \\leq 0$', '$|X_{fake}|$ | $X_{fake} \\leq 0$'])\n",
    "\n",
    "ax.set_xlabel('|X|', size=14)\n",
    "ax.set_ylabel('ECDF', size=14)\n",
    "\n",
    "plt.savefig('./figure/loss_ecdf.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875675a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
