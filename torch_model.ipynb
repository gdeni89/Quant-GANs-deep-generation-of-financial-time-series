{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN MODEL implemented in Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wiese et al., Quant GANs: Deep Generation of Financial Time Series, 2019](https://arxiv.org/abs/1907.06673)\n",
    "\n",
    "For both the generator and the discriminator we used TCNs with skip connections. Inside the TCN architecture temporal blocks were used as block modules. A temporal block consists of two dilated causal convolutions and two PReLUs (He et al., 2015) as activation functions. The primary benefit of using temporal blocks is to make the TCN more expressive by increasing the number of non-linear operations in each block module. A complete definition is given below.\n",
    "\n",
    "**Definition B.1 (Temporal block)**. Let $N_I, N_H, N_O ∈ \\Bbb{N}$ denote the input, hidden and output dimension and let $D,K ∈ \\mathbb{N}$ denote the dilation and the kernel size. Furthermore, let $w_1, w_2$ be two dilated causal convolutional layers with arguments $(N_I, N_H, K, D)$  and $(N_H,N_O,K,D)$ respectively and\n",
    "let $φ_1, φ_2 : \\mathbb{R} → \\mathbb{R}$ be two PReLUs. The function $f : \\mathbb{R}^{N_I×(2D(K−1)+1)} → \\mathbb{R}^{N_O}$ defined by\n",
    "$$f(X) = φ_2 ◦ w_2 ◦ φ_1 ◦ w_1(X)$$\n",
    "is called temporal block with arguments $(N_I,N_H,N_O,K,D)$.\n",
    "\n",
    "The TCN architecture used for the generator and the discriminator in the pure TCN and C-SVNN model is illustrated in Table 3. Table 4 shows the input, hidden and output dimensions of the different models. Here, G abbreviates the generator and D the discriminator. Note that for all models, except the generator of the C-SVNN, the hidden dimension was set to eighty. The kernel size of each temporal block, except the first one, was two. Each TCN modeled a RFS of 127."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-ik58{background-color:#2f2f2f;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<h3>Table 3</h3>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-ik58\">Module Name</th>\n",
    "    <th class=\"tg-ik58\">Arguments</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 1</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 1, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 2</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 3</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 2)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 4</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 4)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 5</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 8)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 6</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 16)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 7</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 32)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">1 x 1 Convolution</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>H</sub>, N<sub>O</sub>, 1, 1)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;background-color:#2f2f2f;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<h3>Table 4</h3>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">Models</th>\n",
    "    <th class=\"tg-0lax\">PureTCN-G</th>\n",
    "    <th class=\"tg-0lax\">Pure TCN-D<br></th>\n",
    "    <th class=\"tg-0lax\">C-SVNN-G</th>\n",
    "    <th class=\"tg-0lax\">C-SVNN_D</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>I</sub></td>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>H</sub></td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "    <td class=\"tg-0lax\">50<br></td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>O</sub></td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"Creates a temporal block.\n",
    "    Args:\n",
    "        n_inputs (int): number of inputs.\n",
    "        n_outputs (int): size of fully connected layers.\n",
    "        kernel_size (int): kernel size along temporal axis of convolution layers within the temporal block.\n",
    "        dilation (int): dilation of convolution layers along temporal axis within the temporal block.\n",
    "        padding (int): padding\n",
    "        dropout (float): dropout rate\n",
    "    Returns:\n",
    "        tuple of output layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        if padding == 0:\n",
    "            self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1, self.conv2, self.relu2, self.dropout2)\n",
    "        else:\n",
    "            self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1, self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.5)\n",
    "        self.conv2.weight.data.normal_(0, 0.5)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return out, self.relu(out + res)\n",
    "\n",
    "pp = 20\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator: 3 to 1 Causal temporal convolutional network with skip connections.\n",
    "       This network uses 1D convolutions in order to model multiple timeseries co-dependency.\n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.tcn = nn.ModuleList([TemporalBlock(3, pp, kernel_size=1, stride=1, dilation=1, padding=0),\n",
    "                                 *[TemporalBlock(pp, pp, kernel_size=2, stride=1, dilation=i, padding=i) for i in [1, 2, 4, 8, 16, 32]]])\n",
    "        self.last = nn.Conv1d(pp, 1, kernel_size=1, stride=1, dilation=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_layers = []\n",
    "        for layer in self.tcn:\n",
    "            skip, x = layer(x)\n",
    "            skip_layers.append(skip)\n",
    "        x = self.last(x + sum(skip_layers))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discrimnator: 1 to 1 Causal temporal convolutional network with skip connections.\n",
    "       This network uses 1D convolutions in order to model multiple timeseries co-dependency.\n",
    "    \"\"\" \n",
    "    def __init__(self, seq_len, conv_dropout=0.05):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.tcn = nn.ModuleList([TemporalBlock(1, pp, kernel_size=1, stride=1, dilation=1, padding=0),\n",
    "                                 *[TemporalBlock(pp, pp, kernel_size=2, stride=1, dilation=i, padding=i) for i in [1, 2, 4, 8, 16, 32]]])\n",
    "        self.last = nn.Conv1d(pp, 1, kernel_size=1, dilation=1)\n",
    "        self.to_prob = nn.Sequential(nn.Linear(seq_len, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_layers = []\n",
    "        for layer in self.tcn:\n",
    "            skip, x = layer(x)\n",
    "            skip_layers.append(skip)\n",
    "        x = self.last(x + sum(skip_layers))\n",
    "        return self.to_prob(x).squeeze()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d89b3f520990f67813a536e0845046cd8eba1f701f32f0e9331279df485a6ae1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
